{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1bd2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\endnj\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\endnj\\AppData\\Local\\Temp\\ipykernel_15940\\2077065600.py:11: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Word2Vec 모델 불러오기\n",
    "word2vec_model = Word2Vec.load(\"model_skipgram.word2vec\")\n",
    "\n",
    "# 전처리된 데이터에서 'TaggedForms'만 사용하여 전체 텍스트 데이터셋 생성\n",
    "# 예시에서는 'processed_results'가 사전에 정의되어 있어야 합니다.\n",
    "with open('processed_results_sample.pickle', 'rb') as handle:\n",
    "    processed_results = pickle.load(handle)\n",
    "text_data = [info['TaggedForms'] for _, info in processed_results.items()]\n",
    "\n",
    "# Keras의 Tokenizer를 사용하여 텍스트 데이터셋 토큰화 및 word_index 생성\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# 모든 시퀀스를 동일한 길이로 패딩\n",
    "max_len = 500  # 시퀀스의 최대 길이 설정\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 임베딩 레이어에 설정할 가중치 매트릭스 준비\n",
    "vocab_size = len(word_index) + 1  # word_index의 크기 + 1\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5fe6f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\endnj\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\endnj\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          7938600   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 100)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               12928     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7951915 (30.33 MB)\n",
      "Trainable params: 7951915 (30.33 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "# Embedding 레이어 생성 및 모델 정의\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # L2 정규화 적용\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # 다중 클래스 분류를 위한 손실 함수\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 요약 출력\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d47daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from mecab import MeCab\n",
    "mecab = MeCab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b7bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "train_df = pd.read_csv(\"train.tsv\", delimiter='\\t')\n",
    "dev_df = pd.read_csv(\"dev.tsv\", delimiter='\\t')\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"배치 처리 로직을 구현합니다. comments 필드에 대해 형태소 분석 및 품사 태깅을 수행합니다.\"\"\"\n",
    "    processed_batch_val = {}\n",
    "    for idx, info in batch.items():\n",
    "        processed_info_val = {\n",
    "            'comments': ' '.join([f'{word}/{tag}' for word, tag in mecab.pos(info['comments'])]),\n",
    "            'contain_gender_bias': info['contain_gender_bias'],\n",
    "            'bias': info['bias'],\n",
    "            'hate': info['hate']\n",
    "        }\n",
    "        processed_batch_val[idx] = processed_info_val\n",
    "    return processed_batch_val\n",
    "\n",
    "# 전처리 함수 적용\n",
    "train_data = {idx: row for idx, row in train_df.iterrows()}\n",
    "dev_data = {idx: row for idx, row in dev_df.iterrows()}\n",
    "processed_train_data = process_batch(train_data)\n",
    "processed_dev_data = process_batch(dev_data)\n",
    "\n",
    "# 전처리된 텍스트 데이터셋 생성\n",
    "train_texts = [info['comments'] for _, info in processed_train_data.items()]\n",
    "dev_texts = [info['comments'] for _, info in processed_dev_data.items()]\n",
    "\n",
    "# Tokenizer를 사용하여 텍스트 데이터셋 토큰화 및 word_index 생성\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_texts)\n",
    "\n",
    "# 모든 시퀀스를 동일한 길이로 패딩\n",
    "train_padded = pad_sequences(train_sequences, maxlen=500, padding='post')\n",
    "dev_padded = pad_sequences(dev_sequences, maxlen=500, padding='post')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541654c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 레이블 인코더 초기화 및 인코딩 수행\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df['hate'])\n",
    "dev_labels_encoded = label_encoder.transform(dev_df['hate'])\n",
    "\n",
    "# 원-핫 인코딩으로 변환\n",
    "train_labels = to_categorical(train_labels_encoded)\n",
    "dev_labels = to_categorical(dev_labels_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e82eb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7896, 500)\n",
      "(471, 500)\n"
     ]
    }
   ],
   "source": [
    "print(train_padded.shape)\n",
    "print(dev_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f37d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 1.127647876739502; accuracy of 55.506330728530884%\n",
      "\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 1.0474457740783691; accuracy of 58.70804190635681%\n",
      "\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 1.0565822124481201; accuracy of 57.948070764541626%\n",
      "\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 1.041219711303711; accuracy of 59.65800881385803%\n",
      "\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 1.0490326881408691; accuracy of 59.59467887878418%\n",
      "\n",
      "Accuracy over all folds: 58.28% (+/- 1.52%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "X = train_padded\n",
    "y = train_labels\n",
    "\n",
    "# 교차 검증을 위한 KFold 설정\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies = []  # 각 폴드의 정확도를 저장할 리스트\n",
    "fold_no = 1\n",
    "\n",
    "for train, test in kfold.split(X, np.argmax(y, axis=1)):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    model.fit(X[train], y[train], epochs=50, batch_size=32, verbose=0, validation_data=(X[test], y[test]))\n",
    "    \n",
    "    # 모델 평가 및 정확도 저장\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%\\n')\n",
    "    accuracies.append(scores[1] * 100)\n",
    "    \n",
    "    fold_no += 1\n",
    "\n",
    "# 평균 정확도와 표준편차 계산 및 출력\n",
    "print(f'Accuracy over all folds: {np.mean(accuracies):.2f}% (+/- {np.std(accuracies):.2f}%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3d33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
